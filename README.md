# Sparkify Data Warehouse

Sparkify, a pretend startup, wants to analyze data on their music streaming app. What songs are people listening to? What artists get the most play time? What type of users are listening to the most music? In order to answer those questions, this project takes JSON files in S3 buckets and transforms them into fact and dimension tables hosted on Redshift. 

## How to use these files locally
1. Download/Fork the repository.
2. Create a dwh.cfg file which include the following three sections and correct key value pairs:
    - \[CLUSTER\]: HOST, DB_NAME, DB_USER, DB_PASSWORD, and DB_PORT
    - \[IAM_ROLE\]: ARN
    - \[S3\]: LOG_DATA, LOG_JSONPATH, SONG_DATA
3. Run create_tables.py. This will connect to the redshift cluster and create the staging, fact, and dimension tables.
4. Run etl.py. This will process that data listed below and add that data to the tables just created.
5. Have fun querying the database :)

## Data in this project:
- **song_data** -- A subset of the [Million Song Dataset](http://millionsongdataset.com). Each file contains metadata about a song and the artist of that song.
- **log_data** -- JSON files generated by [eventsim](https://github.com/Interana/eventsim), which simulate activity logs from a music streaming app based on specified configurations.

## Files in this project:
- **create_tables.py** -- This file drops and creates the database tables. This file is run to reset the tables before each ETL script run.
- **etl.py** -- Reads and processes files from the song_data and log_data S3 buckets and loads them into the database tables.
- **sql_queries.py** -- Contains all SQL queries for the project.